{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e0786-8be5-48a7-901a-19f1263865c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added project root to sys.path: C:\\Users\\viksh\\nanollm\\src\\scripts\n",
      "2025-05-28 15:35:06,405 - INFO - Logging configured within notebook.\n",
      "Error importing project modules: No module named 'nano_llm'\n",
      "Please ensure you launched jupyter lab from the project root directory\n",
      "Current working directory: C:\\Users\\viksh\\nanollm\\src\\scripts\n",
      "Sys path: ['C:\\\\Users\\\\viksh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\python39.zip', 'C:\\\\Users\\\\viksh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\DLLs', 'C:\\\\Users\\\\viksh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib', 'C:\\\\Users\\\\viksh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39', 'C:\\\\Users\\\\viksh\\\\nanollm\\\\.venv', '', 'C:\\\\Users\\\\viksh\\\\nanollm\\\\.venv\\\\lib\\\\site-packages', 'C:\\\\Users\\\\viksh\\\\nanollm\\\\.venv\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\viksh\\\\nanollm\\\\.venv\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\viksh\\\\nanollm\\\\.venv\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\viksh\\\\nanollm\\\\src\\\\scripts']\n"
     ]
    }
   ],
   "source": [
    "# --- Imports and Setup ---\n",
    "# Run this cell first.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import logging # Import logging module\n",
    "\n",
    "# Add the project root to the Python path so we can import nano_llm\n",
    "# This assumes you launched jupyter lab from the project root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added project root to sys.path: {project_root}\")\n",
    "else:\n",
    "    print(\"Project root already in sys.path.\")\n",
    "\n",
    "\n",
    "# Setup basic logging within the notebook\n",
    "# Prevent adding multiple handlers if cell is run multiple times\n",
    "if not logging.getLogger().handlers:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        stream=sys.stdout # Log to console (notebook output)\n",
    "    )\n",
    "    logging.info(\"Logging configured within notebook.\")\n",
    "else:\n",
    "    logging.info(\"Logging already configured.\")\n",
    "\n",
    "\n",
    "# Now you can import your project modules\n",
    "try:\n",
    "    from nano_llm.models import SimpleTransformerDecoder\n",
    "    from nano_llm.data_processing import CharTokenizer # Or your new SubwordTokenizer\n",
    "    from nano_llm.utils import load_checkpoint # Assuming load_checkpoint is in utils.py\n",
    "    import torch.nn.functional as F # For softmax etc if needed manually later\n",
    "    print(\"Successfully imported project modules.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    print(\"Please ensure you launched jupyter lab from the project root directory\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Sys path: {sys.path}\")\n",
    "    # You might need to adjust the sys.path.append line above if your structure is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4089fc53-7e39-4210-9f0d-08c5188e3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Configurations\n",
    "#Load the necessary YAML configuration files for the model, data, and inference settings.\n",
    "#Ensure these file paths are correct relative to where you are running the notebook from (which should be the project root if you followed the setup instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e6a50a-83ca-4cd3-b72f-fab9adb893b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-28 15:35:56,269 - ERROR - Configuration file not found: [Errno 2] No such file or directory: 'config/model_config.yaml'\n",
      "Configuration loading complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Configurations ---\n",
    "# Run this cell.\n",
    "\n",
    "try:\n",
    "    with open('config/model_config.yaml', 'r') as f:\n",
    "        model_config = yaml.safe_load(f)\n",
    "        logging.info(\"Loaded model_config.yaml\")\n",
    "\n",
    "    with open('config/data_config.yaml', 'r') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "        logging.info(\"Loaded data_config.yaml\")\n",
    "\n",
    "    with open('config/inference_config.yaml', 'r') as f:\n",
    "        inference_config = yaml.safe_load(f)\n",
    "        logging.info(\"Loaded inference_config.yaml\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"Configuration file not found: {e}\")\n",
    "    # You might want to stop here if configs are essential\n",
    "except yaml.YAMLError as e:\n",
    "    logging.error(f\"Error parsing configuration file: {e}\")\n",
    "    # You might want to stop here\n",
    "\n",
    "print(\"Configuration loading complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52746511-da91-416a-8ff3-7d6d4b9c8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Device\n",
    "#Set the device (CPU or GPU) to use for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30949c14-869d-4697-8be1-1c450a91fb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-28 15:36:32,613 - INFO - Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Determine Device ---\n",
    "# Run this cell.\n",
    "\n",
    "# Prioritize CUDA if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Optionally, you can try to get device from inference_config, fallback if not available\n",
    "# if 'device' in inference_config and inference_config['device'].lower() != 'auto':\n",
    "#     requested_device = inference_config['device'].lower()\n",
    "#     if requested_device == 'cuda' and not torch.cuda.is_available():\n",
    "#          logging.warning(\"CUDA device requested but not available. Falling back to CPU.\")\n",
    "#          device = torch.device('cpu')\n",
    "#     elif requested_device in ['cuda', 'cpu']:\n",
    "#          device = torch.device(requested_device)\n",
    "#     else:\n",
    "#          logging.warning(f\"Unknown device '{requested_device}' specified in inference_config. Using auto-detected.\")\n",
    "\n",
    "logging.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769476b4-2b1b-4236-aacd-0736cb58047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "#Load the vocabulary and initialize the tokenizer. This is needed to encode your prompt and decode the generated output.\n",
    "#Make sure the `vocab.json` file exists in your processed data directory after running preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f37bec0-030c-4ece-b295-74d34cc95441",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- Load Tokenizer ---\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Run this cell.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m processed_dir_relative \u001b[38;5;241m=\u001b[39m \u001b[43mdata_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_data_dir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/processed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Construct the absolute path to the processed data directory from the project root\u001b[39;00m\n\u001b[0;32m      5\u001b[0m processed_dir_abs \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(project_root, processed_dir_relative)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# --- Load Tokenizer ---\n",
    "# Run this cell.\n",
    "processed_dir_relative = data_config.get('processed_data_dir', 'data/processed')\n",
    "# Construct the absolute path to the processed data directory from the project root\n",
    "processed_dir_abs = os.path.join(project_root, processed_dir_relative)\n",
    "vocab_path = os.path.join(processed_dir_abs, 'vocab.json')\n",
    "\n",
    "try:\n",
    "    # Assuming CharTokenizer has a class method load_vocab\n",
    "    tokenizer = CharTokenizer.load_vocab(vocab_path) # <-- Use your actual Tokenizer class/method\n",
    "    logging.info(f\"Tokenizer loaded from {vocab_path}. Vocab size: {tokenizer.vocab_size}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"Vocabulary file not found at {vocab_path}. Ensure preprocess_data.py has been run.\")\n",
    "    # You cannot proceed without the tokenizer\n",
    "    # You might want to manually stop execution here or handle it.\n",
    "    tokenizer = None # Set tokenizer to None if load fails\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading tokenizer vocabulary from {vocab_path}: {e}\")\n",
    "    tokenizer = None # Set tokenizer to None if load fails\n",
    "\n",
    "\n",
    "if tokenizer is not None:\n",
    "    # You can test encoding/decoding here if the tokenizer loaded successfully\n",
    "    test_text = \"hello world\"\n",
    "    test_ids = tokenizer.encode(test_text)\n",
    "    print(f\"Test encode '{test_text}': {test_ids}\")\n",
    "    print(f\"Test decode {test_ids}: '{tokenizer.decode(test_ids)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43bc2035-0176-46a8-9b33-7588ada8ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference (Text Generation)\n",
    "#Use the loaded model to generate text based on a starting prompt.\n",
    "#Modify the `prompt_text`, `max_new_tokens`, `temperature`, and `top_k` parameters below to experiment with generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0907bc0-d21b-48e8-ab9a-a2c2f94724fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Inference ---\n",
    "# Run this cell.\n",
    "\n",
    "if model is None:\n",
    "    print(\"Cannot run inference because the model was not loaded successfully.\")\n",
    "else:\n",
    "    # Define your prompt and generation parameters\n",
    "    # Get defaults from inference_config\n",
    "    prompt_text = \"The quick brown fox\" # <-- CHANGE THIS PROMPT\n",
    "    max_new_tokens = inference_config.get('max_new_tokens', 100)\n",
    "    temperature = inference_config.get('temperature', 0.8)\n",
    "    top_k = inference_config.get('top_k') # None by default\n",
    "\n",
    "    logging.info(f\"Prompt: '{prompt_text}'\")\n",
    "    logging.info(f\"Generation parameters: max_new_tokens={max_new_tokens}, temperature={temperature}, top_k={top_k}\")\n",
    "\n",
    "\n",
    "    # Encode the prompt\n",
    "    prompt_token_ids = tokenizer.encode(prompt_text)\n",
    "\n",
    "    if not prompt_token_ids:\n",
    "        logging.warning(\"Prompt resulted in empty token list. Cannot generate.\")\n",
    "    else:\n",
    "        # Convert to tensor and move to device\n",
    "        prompt_tensor = torch.tensor([prompt_token_ids], dtype=torch.long, device=device) # Shape (1, prompt_length)\n",
    "        logging.info(f\"Prompt tensor shape: {prompt_tensor.shape}\")\n",
    "\n",
    "        # Disable gradient calculations during generation\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Generate text using the model's generate method\n",
    "                generated_tokens = model.generate(\n",
    "                    prompt_tensor,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    top_k=top_k\n",
    "                )\n",
    "\n",
    "                # Decode the generated tokens\n",
    "                # generated_tokens shape: (batch_size, prompt_length + max_new_tokens)\n",
    "                # Since we use batch_size=1, get the single sequence [0] and convert to list\n",
    "                generated_ids_list = generated_tokens[0].tolist()\n",
    "\n",
    "                # Decode the list of token IDs back into a string\n",
    "                generated_text = tokenizer.decode(generated_ids_list)\n",
    "\n",
    "                print(\"\\n\" + \"=\" * 40)\n",
    "                print(\"Generated Text:\")\n",
    "                print(generated_text)\n",
    "                print(\"=\" * 40 + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during text generation: {e}\")\n",
    "                # Optional: Decode and print partial generation up to the error point\n",
    "                # print(\"Partial generated sequence (before error):\", tokenizer.decode(generated_tokens[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
