# NanoLLM

NanoLLM is a minimal character-level transformer built for educational purposes. It showcases the full pipeline of training a tiny Large Language Model (LLM) from scratch — including data preprocessing, model configuration, training, evaluation, and inference.

> ⚠️ This project is a **learning-focused implementation**. It is not optimized for production or large-scale language modeling.

---

## Key Features

- Character-level transformer model
- Config-driven modular design
- Tokenization, training, evaluation, and inference workflows
- Simple YAML-based configuration system
- Jupyter notebook for experimentation

---

## Project Goals

NanoLLM aims to provide a readable, modular codebase that demonstrates how LLMs work under the hood. It’s ideal for:

- Students and researchers learning about transformers
- Developers prototyping small LLMs
- Anyone curious about end-to-end model training pipelines

---

## Quick Links

- [Installation Guide](installation.md)
- [How to Train & Use the Model](usage.md)
- [Configuration Reference](config_reference.md)
- [API Reference](api_reference.md)
